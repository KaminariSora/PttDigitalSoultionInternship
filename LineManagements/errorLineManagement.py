import json
from pythainlp.tag import pos_tag
from pythainlp.tokenize import word_tokenize
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from sklearn.cluster import DBSCAN
import matplotlib.font_manager as fm

thai_font = fm.FontProperties(fname="C:/Windows/Fonts/tahoma.ttf")

# ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
with open("Data/overlap_testing_file.json", "r", encoding="utf-8") as f:
    json_data = json.load(f)

def visualize_words_and_clustering(word_boxes, lines, page_width, page_height, save_path=None):
    """‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û word boxes ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 12))
    
    # ‡∏†‡∏≤‡∏û‡∏ã‡πâ‡∏≤‡∏¢: ‡πÅ‡∏™‡∏î‡∏á word boxes ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    ax1.set_title("All Word Boxes (Original)", fontsize=14)
    for i, word in enumerate(word_boxes):
        # ‡∏ß‡∏≤‡∏î bounding box
        rect = Rectangle((word["min_x"] * page_width, 
                         (1 - word["max_y"]) * page_height),
                        (word["max_x"] - word["min_x"]) * page_width,
                        (word["max_y"] - word["min_y"]) * page_height,
                        linewidth=1, edgecolor='blue', facecolor='lightblue', alpha=0.3)
        ax1.add_patch(rect)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        ax1.text(word["center_x"] * page_width, 
                (1 - word["center_y"]) * page_height, 
                word["content"], fontproperties=thai_font, fontsize=6, verticalalignment='top')
    
    ax1.set_xlim(0, page_width)
    ax1.set_ylim(0, page_height)
    ax1.set_aspect('equal')
    
    # ‡∏†‡∏≤‡∏û‡∏Ç‡∏ß‡∏≤: ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
    ax2.set_title("Line Clustering Result", fontsize=14)
    colors = plt.cm.tab20(np.linspace(0, 1, len(lines)))
    
    for i, line in enumerate(lines):
        color = colors[i % len(colors)]
        for word in line:
            # ‡∏ß‡∏≤‡∏î bounding box ‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
            rect = Rectangle((word["min_x"] * page_width, 
                             (1 - word["max_y"]) * page_height),
                            (word["max_x"] - word["min_x"]) * page_width,
                            (word["max_y"] - word["min_y"]) * page_height,
                            linewidth=2, edgecolor=color, facecolor=color, alpha=0.3)
            ax2.add_patch(rect)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            ax2.text(word["center_x"] * page_width, 
                    (1 - word["center_y"]) * page_height, 
                    word["content"], fontproperties=thai_font, fontsize=6, verticalalignment='top')
    
    ax2.set_xlim(0, page_width)
    ax2.set_ylim(0, page_height)
    ax2.set_aspect('equal')
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()

def cluster_lines_with_dbscan(word_boxes):
    """‡πÉ‡∏ä‡πâ DBSCAN clustering ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î"""
    if not word_boxes:
        return []
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DBSCAN (‡πÉ‡∏ä‡πâ center_y ‡πÅ‡∏•‡∏∞ height)
    features = []
    for word in word_boxes:
        features.append([word["center_y"], word["height"]])
    
    features = np.array(features)
    
    # ‡πÉ‡∏ä‡πâ DBSCAN clustering
    # eps: ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    # min_samples: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°
    clustering = DBSCAN(eps=0.02, min_samples=1).fit(features)
    
    # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≤‡∏° cluster labels
    clusters = {}
    for i, label in enumerate(clustering.labels_):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(word_boxes[i])
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á Y ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á X
    lines = []
    for cluster_id, words in clusters.items():
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏Å‡∏ô X
        words.sort(key=lambda w: w["center_x"])
        lines.append(words)
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏Å‡∏ô Y (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á center_y ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)
    lines.sort(key=lambda line: sum(w["center_y"] for w in line) / len(line))
    
    return lines

def cluster_lines_improved(word_boxes):
    """‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"""
    if not word_boxes:
        return []
    
    lines = []
    remaining_words = word_boxes.copy()
    
    while remaining_words:
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ Y ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î (‡∏ö‡∏ô‡∏™‡∏∏‡∏î)
        remaining_words.sort(key=lambda w: w["center_y"])
        seed_word = remaining_words.pop(0)
        current_line = [seed_word]
        
        # ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        words_to_remove = []
        for i, word in enumerate(remaining_words):
            if is_same_line(seed_word, word, current_line):
                current_line.append(word)
                words_to_remove.append(i)
        
        # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å remaining_words
        for i in sorted(words_to_remove, reverse=True):
            remaining_words.pop(i)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏Å‡∏ô X
        current_line.sort(key=lambda w: w["center_x"])
        lines.append(current_line)
    
    return lines

def is_same_line(reference_word, test_word, current_line):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏™‡∏≠‡∏á‡∏Ñ‡∏≥‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    avg_y = sum(w["center_y"] for w in current_line) / len(current_line)
    avg_height = sum(w["height"] for w in current_line) / len(current_line)
    
    # ‡πÄ‡∏ä‡πá‡∏Ñ vertical overlap
    ref_top = reference_word["min_y"]
    ref_bottom = reference_word["max_y"]
    test_top = test_word["min_y"]
    test_bottom = test_word["max_y"]
    
    overlap_top = max(ref_top, test_top)
    overlap_bottom = min(ref_bottom, test_bottom)
    overlap_height = max(0, overlap_bottom - overlap_top)
    
    min_height = min(reference_word["height"], test_word["height"])
    overlap_ratio = overlap_height / min_height if min_height > 0 else 0
    
    # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°
    y_distance = abs(test_word["center_y"] - avg_y)
    
    # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç 1: ‡∏°‡∏µ overlap ratio ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
    has_enough_overlap = overlap_ratio > 0.4
    
    # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç 2: ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á Y ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
    reasonable_distance = y_distance < (avg_height * 0.4)
    
    return has_enough_overlap and reasonable_distance

output_lines = []
total_words = 0
total_words_filtered = 0 

for page_idx, page in enumerate(json_data["analyzeResult"]["pages"]):
    print(f"\n=== ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_idx + 1} ===")
    
    words = page["words"]
    page_width = page["width"]
    page_height = page["height"]

    word_boxes = []
    for word in words:
        # ‡∏ñ‡πâ‡∏≤ content ‡∏°‡∏µ \n ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢
        if "\n" in word["content"]:
            parts = [p.strip() for p in word["content"].split("\n") if p.strip()]
        else:
            parts = [word["content"]]

        polygon = word["polygon"]
        x_coords = polygon[::2]
        y_coords = polygon[1::2]

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á bounding box
        min_x = min(x_coords) / page_width
        max_x = max(x_coords) / page_width
        min_y = min(y_coords) / page_height
        max_y = max(y_coords) / page_height
        
        center_x = (min_x + max_x) / 2
        center_y = (min_y + max_y) / 2
        height = max_y - min_y

        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥ (‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏¢‡πà‡∏≠‡∏¢‡∏à‡∏≤‡∏Å \n)
        for part in parts:
            word_boxes.append({
                "content": part,
                "center_x": center_x,
                "center_y": center_y,
                "min_x": min_x,
                "max_x": max_x,
                "min_y": min_y,
                "max_y": max_y,
                "height": height
            })

    # ‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏ß‡∏¥‡∏ò‡∏µ
    print("\n--- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Improved Method ---")
    lines_improved = cluster_lines_improved(word_boxes)
    
    print("\n--- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö DBSCAN Method ---")
    lines_dbscan = cluster_lines_with_dbscan(word_boxes)
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)
    if abs(len(lines_improved) - 8) < abs(len(lines_dbscan) - 8):  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ 8 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
        lines = lines_improved
        method_used = "Improved"
    else:
        lines = lines_dbscan
        method_used = "DBSCAN"
    
    print(f"\nüéØ ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ: {method_used}")
    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö: {len(lines)}")

    # ‡πÅ‡∏™‡∏î‡∏á visualization
    visualize_words_and_clustering(word_boxes, lines, page_width, page_height, 
                                 f"Data/clustering_result_page_{page_idx + 1}.png")

    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    for line_idx, line in enumerate(lines):
        line_text = ""
        for i, word in enumerate(line):
            line_text += word["content"]
            if i < len(line) - 1:
                next_word = line[i + 1]
                space = next_word["center_x"] - word["center_x"]
                if space > 0.06:  # threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ
                    line_text += "_"

        # Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
        y_positions = [w["center_y"] for w in line]
        heights = [w["height"] for w in line]
        
        print(f"‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î {line_idx + 1}: {line_text}")
        print(f"  - Y range: {min(y_positions):.4f} - {max(y_positions):.4f}")
        print(f"  - Height range: {min(heights):.4f} - {max(heights):.4f}")
        print(f"  - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥: {len(line)}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö overlap ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
        if line_idx > 0:
            prev_line = lines[line_idx - 1]
            prev_y_max = max(w["max_y"] for w in prev_line)
            curr_y_min = min(w["min_y"] for w in line)
            if prev_y_max > curr_y_min:
                print(f"  ‚ö†Ô∏è  ‡∏°‡∏µ overlap ‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {prev_y_max:.4f} > {curr_y_min:.4f}")
        print()

        tokens = word_tokenize(line_text.replace("_", " "), keep_whitespace=False)
        tags = pos_tag(tokens, engine="perceptron", corpus="orchid")

        filtered_tokens = [word for word, tag in tags if tag not in ["NPRP", "NTTL"]]

        word_count = len(tokens)
        word_count_filtered = len(filtered_tokens)

        total_words += word_count
        total_words_filtered += word_count_filtered

        output_lines.append(line_text)

print(f"\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•:")
print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á: {total_words}")
print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á: {total_words_filtered}")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå txt
with open("Data/output_sorted_lines.txt", "w", encoding="utf-8") as out_file:
    for line in output_lines:
        out_file.write(line + "\n")

print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: Data/output_sorted_lines.txt")
print("üé® ‡∏†‡∏≤‡∏û visualization ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: Data/clustering_result_page_*.png")